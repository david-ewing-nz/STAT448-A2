---
title: "STAT448 Assignment 2 – Regression Modelling Across Domains"
author:
  - "David Ewing (82171165)"
  - "Lillian Lee (32198314)"
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: xelatex
    fig_caption: true
    number_sections: true
    toc: true
    keep_tex: false
    fig_crop: false
fontsize: 11pt
geometry: margin=1in
---

```{r setup, include=FALSE}
# This chunk was set up with the aid of ChatGPT.
# The intent is to load updates quietly thus not
# spending undue time with the logistics of getting 
# setup. 


#---------------- loading libraries ---------------------------
#---------------- loading libraries ---------------------------
# Load configuration for reproducibility and preferred CRAN mirror
options(repos = c(CRAN = "https://cran.stat.auckland.ac.nz/"))

# Required packages
library(conflicted) # before any other

required_packages <- c(
  "broom",         # Model tidying
  "caret",         # Model training utilities
  "class",         # kNN
  "cowplot",       # Plot composition
  "dplyr",         # Data wrangling
  "flextable",     # Summary tables
  "GGally",        # Pair plots
  "ggplot2",       # Core plotting
  "glmnet",        # Regularised regression
  "kableExtra",    # Table formatting
  "knitr",         # Inline rendering
  "MASS",          # LDA/QDA and logistic
  "purrr",         # List/vector formatting
  "readxl",        # XLSX reading
  "rlang",         # Expression handling
  "skimr",         # Data summaries
  "tidyverse"      # Core data science packages
)

for (pkg in required_packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
  library(pkg, character.only = TRUE)
}

#---------------- conflict_prefer ------------------------------
#---------------- conflict_prefer ------------------------------
conflict_prefer("filter", "dplyr"); conflict_prefer("select", "dplyr")

set.seed(82171165)
knitr::opts_chunk$set(dev.args = list(png = list(type = "cairo")))


# ---- Pretty Summary  with Type Summary and Preview ------
# ---- Pretty Summary  with Type Summary and Preview ------
# works with flextable; provides a summary of column types
# and a preview

pretty_summary <- \(df) {   # works with flextable 
  select(df, where(is.numeric)) %>%
    summarise(
      across(
        everything(),
        .fns = list(
          Mean     = \(x) mean(x, na.rm = TRUE),
          Median   = \(x) median(x, na.rm = TRUE),
          Min      = \(x) min(x, na.rm = TRUE),
          Max      = \(x) max(x, na.rm = TRUE),
          IQR      = \(x) IQR(x, na.rm = TRUE),
          nNA      = \(x) sum(is.na(x))
        )
      )
    ) %>%
    pivot_longer(
      cols = everything(),
      names_to = c("Variable", ".value"),
      names_sep = "_"
    ) %>%
    mutate(across(where(is.numeric), round, 2)) %>%
    flextable() %>%
    set_header_labels(
      Variable = "Feature",
      Mean     = "Mean",
      Median   = "Median",
      Min      = "Min",
      Max      = "Max",
      IQR      = "Interquartile Range",
      nNA      = "Missing Values"
    ) %>%
    autofit() %>%
    theme_vanilla()
}

# ---- Pretty csv_read with Type Summary and Preview ------
# ---- Pretty csv_read with Type Summary and Preview ------ 
# works with flextable; provides a summary of column types
# and a preview

pretty_read_csv <- \(path, n = 0) {                   # to work with flextable
  df <- readr::read_csv(path, show_col_types = FALSE)
  types <- purrr::map_chr(df, typeof)
  type_df <- data.frame(Column = names(df), Type = types, stringsAsFactors = FALSE)

  print(flextable::flextable(type_df) |>
          flextable::autofit() |>
          flextable::set_caption(caption = "Column Types in Loaded CSV"))

  if (n > 0) print(head(df, n))
  return(df)
}

# ---- Pretty Excel Reader with Type Summary and Preview ------
# Uses flextable for consistent formatting with pretty_read_csv()

pretty_read_xlsx <- \(path, sheet = 1, n = 0) {
  # Read Excel file (defaults to first sheet)
  df <- readxl::read_excel(path, sheet = sheet)
  
  # Summarise column types
  types <- purrr::map_chr(df, typeof)
  type_df <- data.frame(Column = names(df), Type = types, stringsAsFactors = FALSE)
  
  # Display types in a flextable
  print(flextable::flextable(type_df) |>
          flextable::autofit() |>
          flextable::set_caption(caption = "Column Types in Loaded XLSX"))
  
  # Optionally preview first n rows
  if (n > 0) print(head(df, n))
  
  return(df)
}


pretty_df <- function(df, title = NULL, fontsize = 10, max_rows = 20) {
  df_show <- head(df, max_rows)  # Limit to first `max_rows` rows for readability
  
  ft <- flextable(df_show) %>%
    fontsize(size = fontsize, part = "all") %>%
    autofit() %>%
    theme_booktabs() %>%
    align(align = "center", part = "all") %>%
    padding(padding = 5, part = "all") 
  
  if (!is.null(title)) {
    ft <- set_caption(ft, caption = title)
  }
  
  return(ft)
}
 

pretty_ggplot <- function(plot, title = "ggplot Summary") {
  if (!inherits(plot, "gg")) stop("Input must be a ggplot object.")

  # Extract key components
  plot_data <- tryCatch(plot$data, error = function(e) NULL)
  geoms <- sapply(plot$layers, function(layer) class(layer$geom)[1])
  mappings <- plot$mapping
  
  # Pull out global aesthetics as strings
  global_aes <- names(mappings)
  global_vals <- sapply(mappings, function(x) rlang::expr_text(x))

  # Additional metadata
  plot_title <- plot$labels$title %||% ""
  x_lab <- plot$labels$x %||% ""
  y_lab <- plot$labels$y %||% ""
  colour_lab <- plot$labels$colour %||% plot$labels$color %||% ""

  # Build a metadata data frame
  df <- data.frame(
    Component = c("Title", "X Axis", "Y Axis", "Colour Legend", "Geoms", global_aes),
    Value = c(plot_title, x_lab, y_lab, colour_lab, paste(geoms, collapse = ", "), global_vals),
    stringsAsFactors = FALSE
  )

  # Format with flextable
  flextable(df) %>%
    set_caption(caption = title) %>%
    autofit() %>%
    theme_booktabs() %>%
    fontsize(size = 10, part = "all") %>%
    align(align = "left", part = "all") %>%
    padding(padding = 4)
}


pretty_model <- function(model, title = "Model Summary", digits = 3) {
  if (!inherits(model, "glm")) stop("Only 'glm' objects are currently supported.")
  
  # Use broom::tidy() for clean coefficient table
  tidy_tbl <- broom::tidy(model) %>%
    mutate(across(where(is.numeric), ~ round(.x, digits)))

  # Optionally include model fit stats (like AIC, null deviance, etc.)
  stats_tbl <- data.frame(
    Metric = c("Formula", "AIC", "Null deviance", "Residual deviance"),
    Value = c(
      as.character(formula(model)),
      round(AIC(model), digits),
      round(model$null.deviance, digits),
      round(model$deviance, digits)
    ),
    stringsAsFactors = FALSE
  )
  
  # Format coefficient table
  ft_coef <- flextable(tidy_tbl) %>%
    set_caption(caption = title) %>%
    autofit() %>%
    theme_booktabs() %>%
    fontsize(size = 10, part = "all") %>%
    align(align = "center", part = "all") %>%
    padding(padding = 4, part = "all")

  # Format stats table separately (optional)
  ft_stats <- flextable(stats_tbl) %>%
    set_caption(caption = paste(title, "(Fit Stats)")) %>%
    autofit() %>%
    theme_booktabs() %>%
    fontsize(size = 9, part = "all") %>%
    align(align = "left", part = "all") %>%
    padding(padding = 4, part = "all")
  
  # Return both as a list for separate rendering
  list(Coefficients = ft_coef, Stats = ft_stats)
}

sort_column_lables <- function(df) {
  first_row <- df[1, ]     # Row 1 header
  base_names <- names(df)  # Original column names from Excel
  
  # Build new short names and labels
  short_names <- character(length(base_names))
  long_labels <- character(length(base_names))

  for (i in seq_along(base_names)) {
    candidate1 <- base_names[i]
    candidate2 <- as.character(first_row[[i]])

    # boolean array of whether to swap or not
    is_code <- function(x) grepl("^V-?\\d+$|^V\\d+$", x, ignore.case = TRUE)
    
    # ^V-?\\d+$
    #  ^   = Start of the string
    #  V   = Must begin with a "V"
    # -?   = Followed by an optional dash (- may or may not be present)
    # \\d+ = One or more digits
    # $    = End of the string
    

    if (is_code(candidate1) & !is_code(candidate2)) {
      short_names[i] <- candidate1
      long_labels[i] <- candidate2
    } else if (!is_code(candidate1) & is_code(candidate2)) {
      short_names[i] <- candidate2
      long_labels[i] <- candidate1
    } else {
      # fallback if both are long or both are short
      short_names[i] <- candidate2
      long_labels[i] <- candidate1
    }
  }

  # Assign new short names
  names(df) <- short_names

  # Remove first data row used for naming
  df <- df[-1, ]
  rownames(df) <- NULL

  # Attach long labels
  for (i in seq_along(df)) {
    attr(df[[i]], "label") <- long_labels[i] # attach long label
  }

  return(df)
}



```
```{r load-data, message=FALSE, warning=FALSE}
# Define the base path to the data directory
# Load CSV files using pretty_read_csv()
# Load Excel file using pretty_read_xlsx()
data_dir <- "../data/"

parkinsons_df <- pretty_read_csv(file.path(data_dir,  "parkinsons.csv"))
weather_df    <- pretty_read_csv(file.path(data_dir,  "Weather_Station_data_v1.csv"))
#residen_df   <- pretty_read_csv(file.path(data_dir,  "residen.csv"))
residenxl_df  <- pretty_read_xlsx(file.path(data_dir, "Residential-Building-Data-Set.xlsx"))
```
# STAT448 Assignment 2 – Regression Modelling Across Domains

This report addresses the three components of STAT448 Assignment 2, each focused on modelling using a different dataset. The project involves building, evaluating, and interpreting regression models using tools such as linear regression, stepwise selection, and regularised regression.



```{r child = '../doc/01_residential_modelling2.Rmd'}
```

#```{r child = 'doc/02_parkinsons_modelling.Rmd'}
#```

#```{r child = 'doc/03_weather_modelling.Rmd'}
#```

## Conclusion

This report presented three focused modelling tasks corresponding to different datasets and modelling requirements. Each section includes analysis, model fitting, validation, and interpretation appropriate to the data and modelling technique.

Please refer to each child document for details on preprocessing, model evaluation, and final conclusions.